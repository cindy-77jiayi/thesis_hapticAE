{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1",
      "metadata": {},
      "source": [
        "# Haptic Signal VAE — Colab Training\n",
        "\n",
        "This notebook runs the full training pipeline on Google Colab.\n",
        "\n",
        "**Data source:** Cloned directly from [HapticGen/hapticgen-dataset](https://github.com/HapticGen/hapticgen-dataset) — no Google Drive needed for data.\n",
        "\n",
        "**Steps:**\n",
        "1. Clone code repo + dataset repo from GitHub\n",
        "2. Install dependencies\n",
        "3. Run training\n",
        "4. Evaluate and listen to results\n",
        "5. (Optional) Save outputs to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "id": "a2",
      "metadata": {},
      "source": [
        "# 1. Clone code repo (or pull latest)\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/cindy-77jiayi/thesis_hapticAE.git\"\n",
        "REPO_DIR = \"/content/thesis_hapticAE\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    !cd {REPO_DIR} && git pull\n",
        "else:\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a3",
      "metadata": {},
      "source": [
        "# 2. Clone dataset directly from GitHub (no Google Drive needed)\n",
        "DATASET_URL = \"https://github.com/HapticGen/hapticgen-dataset.git\"\n",
        "DATASET_DIR = \"/content/hapticgen-dataset\"\n",
        "\n",
        "if os.path.exists(DATASET_DIR):\n",
        "    !cd {DATASET_DIR} && git pull\n",
        "else:\n",
        "    !git clone {DATASET_URL} {DATASET_DIR}\n",
        "\n",
        "# Data path for training\n",
        "DATA_DIR = os.path.join(DATASET_DIR, \"expertvoted\")\n",
        "print(f\"Dataset: {DATA_DIR}\")\n",
        "print(f\"Files: {len(os.listdir(DATA_DIR)) if os.path.exists(DATA_DIR) else 'NOT FOUND'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a4",
      "metadata": {},
      "source": [
        "# 3. Install dependencies\n",
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a5",
      "metadata": {},
      "source": [
        "# 4. Configure paths\n",
        "OUTPUT_DIR = \"/content/outputs\"  # Colab local storage (fast)\n",
        "CONFIG = \"configs/vae_default.yaml\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Data:   {DATA_DIR}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"Config: {CONFIG}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a6",
      "metadata": {},
      "source": [
        "# 5. Run training\n",
        "!python scripts/train.py --config {CONFIG} --data_dir {DATA_DIR} --output_dir {OUTPUT_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a7",
      "metadata": {},
      "source": [
        "# 6. Evaluate: load results and visualize\n",
        "import sys\n",
        "sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from src.utils.config import load_config\n",
        "from src.utils.seed import set_seed\n",
        "from src.data.preprocessing import collect_clean_wavs, estimate_global_rms\n",
        "from src.data.dataset import HapticWavDataset\n",
        "from src.models.conv_vae import ConvVAE\n",
        "from src.eval.evaluate import evaluate_reconstruction, print_metrics\n",
        "from src.eval.visualize import plot_loss_curves, plot_waveform_comparison\n",
        "from src.eval.audio import play_ab_comparison\n",
        "\n",
        "config = load_config(CONFIG)\n",
        "set_seed(config['seed'])\n",
        "\n",
        "# Find latest run\n",
        "run_dirs = sorted(glob.glob(f\"{OUTPUT_DIR}/*/best_model.pt\"))\n",
        "assert run_dirs, \"No trained models found\"\n",
        "ckpt_path = run_dirs[-1]\n",
        "run_dir = os.path.dirname(ckpt_path)\n",
        "print(f\"Using checkpoint: {ckpt_path}\")\n",
        "\n",
        "# Load metrics\n",
        "metrics = np.load(os.path.join(run_dir, 'metrics.npz'))\n",
        "plot_loss_curves(metrics['train_losses'].tolist(), metrics['val_losses'].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a8",
      "metadata": {},
      "source": [
        "# 7. Reconstruction evaluation + audio\n",
        "data_cfg = config['data']\n",
        "wav_files = collect_clean_wavs(DATA_DIR)\n",
        "N = len(wav_files)\n",
        "perm = np.random.permutation(N)\n",
        "split = int(data_cfg['train_split'] * N)\n",
        "val_files = [wav_files[i] for i in perm[split:]]\n",
        "train_files = [wav_files[i] for i in perm[:split]]\n",
        "global_rms = estimate_global_rms(train_files, n=200, sr_expect=data_cfg['sr'])\n",
        "\n",
        "val_ds = HapticWavDataset(val_files, T=data_cfg['T'], sr_expect=data_cfg['sr'], global_rms=global_rms, scale=data_cfg['scale'])\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_cfg = config['model']\n",
        "model = ConvVAE(\n",
        "    T=data_cfg['T'], latent_dim=model_cfg['latent_dim'],\n",
        "    channels=tuple(model_cfg['channels']),\n",
        "    first_kernel=model_cfg.get('first_kernel', 25),\n",
        "    kernel_size=model_cfg.get('kernel_size', 9),\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "result = evaluate_reconstruction(model, val_loader, device, n_samples=10)\n",
        "print_metrics(result)\n",
        "plot_waveform_comparison(result['x_np'], result['xhat_np'])\n",
        "play_ab_comparison(result['x_np'], result['xhat_np'], sr=data_cfg['sr'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a9",
      "metadata": {},
      "source": [
        "---\n",
        "## (Optional) Save outputs to Google Drive\n",
        "\n",
        "Colab local storage is wiped when the session ends. Run the cell below to copy your trained model to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "id": "a10",
      "metadata": {},
      "source": [
        "# Optional: save outputs to Google Drive for persistence\n",
        "SAVE_TO_DRIVE = False  # Set to True if you want to save\n",
        "\n",
        "if SAVE_TO_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_OUTPUT = \"/content/drive/MyDrive/thesis/outputs\"\n",
        "    !cp -r {OUTPUT_DIR}/* {DRIVE_OUTPUT}/\n",
        "    print(f\"Saved to: {DRIVE_OUTPUT}\")\n",
        "else:\n",
        "    print(\"Skipped. Set SAVE_TO_DRIVE = True to save to Google Drive.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
