{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1",
      "metadata": {},
      "source": [
        "# Haptic Signal VAE â€” Colab Training\n",
        "\n",
        "This notebook runs the full training pipeline on Google Colab.\n",
        "\n",
        "**Steps:**\n",
        "1. Mount Google Drive\n",
        "2. Clone the GitHub repo\n",
        "3. Install dependencies\n",
        "4. Run training via CLI\n",
        "5. Evaluate and listen to results"
      ]
    },
    {
      "cell_type": "code",
      "id": "a2",
      "metadata": {},
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a3",
      "metadata": {},
      "source": [
        "# 2. Clone repo (or pull latest if already cloned)\n",
        "import os\n",
        "\n",
        "REPO_URL = \"https://github.com/cindy-77jiayi/thesis_hapticAE.git\"\n",
        "REPO_DIR = \"/content/thesis_hapticAE\"\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    !cd {REPO_DIR} && git pull\n",
        "else:\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a4",
      "metadata": {},
      "source": [
        "# 3. Install dependencies\n",
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a5",
      "metadata": {},
      "source": [
        "# 4. Configure paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/hapticgen-dataset/expertvoted\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/thesis/outputs\"\n",
        "CONFIG = \"configs/vae_default.yaml\"\n",
        "\n",
        "print(f\"Data:   {DATA_DIR}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"Config: {CONFIG}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a6",
      "metadata": {},
      "source": [
        "# 5. Run training\n",
        "!python scripts/train.py --config {CONFIG} --data_dir {DATA_DIR} --output_dir {OUTPUT_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a7",
      "metadata": {},
      "source": [
        "# 6. Evaluate: load results and visualize\n",
        "import sys\n",
        "sys.path.insert(0, REPO_DIR)\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from src.utils.config import load_config\n",
        "from src.utils.seed import set_seed\n",
        "from src.data.preprocessing import collect_clean_wavs, estimate_global_rms\n",
        "from src.data.dataset import HapticWavDataset\n",
        "from src.models.conv_vae import ConvVAE\n",
        "from src.eval.evaluate import evaluate_reconstruction, print_metrics\n",
        "from src.eval.visualize import plot_loss_curves, plot_waveform_comparison\n",
        "from src.eval.audio import play_ab_comparison\n",
        "\n",
        "config = load_config(CONFIG)\n",
        "set_seed(config['seed'])\n",
        "\n",
        "# Find latest run\n",
        "run_dirs = sorted(glob.glob(f\"{OUTPUT_DIR}/*/best_model.pt\"))\n",
        "assert run_dirs, \"No trained models found\"\n",
        "ckpt_path = run_dirs[-1]\n",
        "run_dir = os.path.dirname(ckpt_path)\n",
        "print(f\"Using checkpoint: {ckpt_path}\")\n",
        "\n",
        "# Load metrics\n",
        "metrics = np.load(os.path.join(run_dir, 'metrics.npz'))\n",
        "plot_loss_curves(metrics['train_losses'].tolist(), metrics['val_losses'].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "a8",
      "metadata": {},
      "source": [
        "# 7. Reconstruction evaluation + audio\n",
        "data_cfg = config['data']\n",
        "wav_files = collect_clean_wavs(DATA_DIR)\n",
        "N = len(wav_files)\n",
        "perm = np.random.permutation(N)\n",
        "split = int(data_cfg['train_split'] * N)\n",
        "val_files = [wav_files[i] for i in perm[split:]]\n",
        "train_files = [wav_files[i] for i in perm[:split]]\n",
        "global_rms = estimate_global_rms(train_files, n=200, sr_expect=data_cfg['sr'])\n",
        "\n",
        "val_ds = HapticWavDataset(val_files, T=data_cfg['T'], sr_expect=data_cfg['sr'], global_rms=global_rms, scale=data_cfg['scale'])\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_cfg = config['model']\n",
        "model = ConvVAE(\n",
        "    T=data_cfg['T'], latent_dim=model_cfg['latent_dim'],\n",
        "    channels=tuple(model_cfg['channels']),\n",
        "    first_kernel=model_cfg.get('first_kernel', 25),\n",
        "    kernel_size=model_cfg.get('kernel_size', 9),\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "result = evaluate_reconstruction(model, val_loader, device, n_samples=10)\n",
        "print_metrics(result)\n",
        "plot_waveform_comparison(result['x_np'], result['xhat_np'])\n",
        "play_ab_comparison(result['x_np'], result['xhat_np'], sr=data_cfg['sr'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
